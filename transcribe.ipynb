{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use audio fragment for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Set the paths to your ffmpeg and ffprobe executables\n",
    "# AudioSegment.ffmpeg = \"/opt/homebrew/bin/ffmpeg\"\n",
    "# AudioSegment.ffprobe = \"/opt/homebrew/bin/ffprobe\"\n",
    "os.environ[\"PATH\"] += f\"{os.pathsep}/opt/homebrew/bin\"\n",
    "\n",
    "# Load the input MP3 file\n",
    "input_file = \"./podcasts/the_home_run/How to prepare yourself to bid at an auction.mp3\"\n",
    "output_file = \"./podcasts/dev.mp3\"\n",
    "audio = AudioSegment.from_mp3(input_file)\n",
    "\n",
    "# Slice the first minute (60,000 milliseconds)\n",
    "first_minute = audio[:60_000]\n",
    "\n",
    "# Export the sliced audio as a new MP3 file\n",
    "first_minute.export(output_file, format=\"mp3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "import os\n",
    "\n",
    "transcript_col = 'text'\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=os.environ['OPEN_API_KEY'])\n",
    "llm = OpenAI(openai_api_key=os.environ['OPEN_API_KEY'])\n",
    "vectordb_persist_dir = 'db'\n",
    "vectordb = Chroma(embedding_function=embedding_function, persist_directory=vectordb_persist_dir)\n",
    "merge_threshold = 2\n",
    "\n",
    "def merge_adjacent_utterances(df):\n",
    "    # Merge records\n",
    "    merged_records = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['merge']:\n",
    "            # Merge text with the next recordb\n",
    "            row['text'] += df.loc[_, 'text']\n",
    "            row['end'] = df.loc[_, 'end']\n",
    "            # Remove the next record from the dataframe\n",
    "            df.drop(_, inplace=True)\n",
    "        merged_records.append(row)\n",
    "    return pd.DataFrame(merged_records).drop(columns=['delta', 'merge']).reset_index(drop=True)\n",
    "\n",
    "def parse_transcript(transcript_file):\n",
    "    dff = (pd.read_csv(transcript_file)\n",
    "    # round, calculate deltas\n",
    "    .assign(start=lambda x: round(x.start, 2))\n",
    "    .assign(end=lambda x: round(x.end, 2))\n",
    "    .assign(delta=lambda x: x.start.shift(-1) - x.end)\n",
    "    .assign(merge=lambda x: x.delta > merge_threshold)\n",
    "    )\n",
    "    return merge_adjacent_utterances(dff)\n",
    "\n",
    "def estimate_cost_of_ingest(transcript_df):\n",
    "    ada_cost_per_1000_tokens = 0.0004\n",
    "    n_tokens = len([e for e in ''.join(transcript_df.text.tolist()).split(' ') if e])\n",
    "    print(f\"{n_tokens} tokens found in transcript\")\n",
    "    cost_estimate = (n_tokens / 1000) * ada_cost_per_1000_tokens\n",
    "    print(f\"Estimate ingestion cost: US ${cost_estimate}\")\n",
    "\n",
    "def ingest_transcript_df(transcript_df):\n",
    "    # TODO: add upsert, prevent redundant writes\n",
    "    documents = DataFrameLoader(transcript_df.head(20), page_content_column=transcript_col).load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # embed, write to chroma, save chroma db\n",
    "    vectordb.add_documents(texts)\n",
    "    vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting transcript: 517_tips_for_choosing_a_builder.csv\n",
      "3226 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0012904000000000001\n",
      "3226 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0012904000000000001\n",
      "Ingesting transcript: australia's_richest_postcodes,_retreat_from_the_regions_&_new_housing_stimulus.csv\n",
      "6265 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.002506\n",
      "6265 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.002506\n",
      "Ingesting transcript: 360._new_cpd_requirements_for_strata_professionals.csv\n",
      "3483 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0013932\n",
      "3483 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0013932\n",
      "Ingesting transcript: the_art_of_persuasion__how_to_get_what_you_want_in_property_deals.csv\n",
      "12225 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.00489\n",
      "12225 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.00489\n",
      "Ingesting transcript: buying_a_home_for_the_future__sustainability_and_long-term_value.csv\n",
      "10843 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0043372\n",
      "10843 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0043372\n",
      "Ingesting transcript: how_to_prepare_yourself_to_bid_at_an_auction.csv\n",
      "5147 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0020588000000000004\n",
      "5147 tokens found in transcript\n",
      "Estimate ingestion cost: US $0.0020588000000000004\n"
     ]
    }
   ],
   "source": [
    "from transcribe import transcript_dir\n",
    "\n",
    "for transcript in list(transcript_dir.rglob('*/*.csv')):\n",
    "    transcript_df = parse_transcript(transcript).assign(podcast=transcript.parent.name)\n",
    "    print(f\"Ingesting transcript: {transcript.name}\")\n",
    "    estimate_cost_of_ingest(transcript_df)\n",
    "    ingest_transcript_df(transcript_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes, this podcast is useful for you if you want to buy your first unit, as it provides tips and tools for first home buyers.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=vectordb)\n",
    "query = \"Is this podcast useful for me if I want to buy my first unit, I am a first home buyer\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
