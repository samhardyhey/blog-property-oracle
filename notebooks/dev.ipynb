{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.concat([pd.read_csv(e) for e in glob.glob(\"../data/meta/*\")])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.read_csv(\n",
    "    \"../data/transcripts/australian_property_podcast/2_sense_builders_going_bankrupt_what_the_rba_reshuffle_means_to_you_the_mortgage_dilemma.csv\"\n",
    ").head(10)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/transcripts/australian_property_podcast/2_sense_builders_going_bankrupt_what_the_rba_reshuffle_means_to_you_the_mortgage_dilemma.csv\"\n",
    "from ingest import MERGE_THRESHOLD\n",
    "\n",
    "pd.read_csv(file)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from property_oracle.llm import embedding_config, llm\n",
    "\n",
    "llm\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic question answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import llm, questions\n",
    "from vectore_store.chroma import vectordb\n",
    "\n",
    "# how many docs in underlying DB\n",
    "vectordb._client._count(\"langchain\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import VectorDBQA\n",
    "\n",
    "from vectore_store.chroma import vectordb\n",
    "\n",
    "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=vectordb)\n",
    "print(qa.run(questions[3]).strip())\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA with source references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever()\n",
    ")\n",
    "res = chain(\n",
    "    {\"question\": questions[0]},\n",
    "    # return_only_outputs=True,\n",
    ")\n",
    "res\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/answer with custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the context below to write a 100 word paragraph response to the question:\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "\n",
    "def question_answer_custom_prompt(question):\n",
    "    docs = vectordb.similarity_search(question, k=10)\n",
    "    return docs\n",
    "    inputs = [{\"context\": doc.page_content, \"question\": question} for doc in docs]\n",
    "    return chain.apply(inputs)\n",
    "\n",
    "\n",
    "res = question_answer_custom_prompt(questions[-1])\n",
    "res\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with docs with memory\n",
    "- use as main entry point > CLI program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), memory=memory)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What important things should I be looking for when inspecting properties?\"\n",
    "result = qa({\"question\": query})\n",
    "\n",
    "query = \"How much should a building and pest inspection typically cost?\"\n",
    "result = qa({\"question\": query})\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect preliminary retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"What are the basic steps I need to complete before attempting to purchase property in Australia?\"\n",
    "docs = vectordb.similarity_search(questions[0], k=10)\n",
    "docs\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Summarize the below context in a bullet-pointed, 50 word technical analysis in response to the question. Be sure to group related bits of content into thematically relevant sections:\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "def question_answer_custom_prompt(question):\n",
    "    docs = vectordb.similarity_search(question, k=10)\n",
    "    # return docs\n",
    "    return chain.run(docs)\n",
    "    # inputs = [{\"context\": doc.page_content, \"question\": question} for doc in docs]\n",
    "    # return chain.apply(inputs)\n",
    "\n",
    "\n",
    "res = question_answer_custom_prompt(questions[0])\n",
    "res\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
